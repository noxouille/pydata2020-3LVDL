{
  
    
        "post0": {
            "title": "Level 1 - Image Classification",
            "content": "Compiled by: Ferdi Pratama . GPU check . First, simply select &quot;GPU&quot; in the Accelerator drop-down in Notebook Settings (either through the Edit menu or the command palette at cmd/ctrl-shift-P). . Then, we should test if the GPU is detected, and find out which GPU we are using. . !nvidia-smi . Sun Apr 5 14:09:07 2020 +--+ | NVIDIA-SMI 440.64.00 Driver Version: 418.67 CUDA Version: 10.1 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 | | N/A 34C P0 26W / 250W | 0MiB / 16280MiB | 0% Default | +-+-+-+ +--+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +--+ . Overview . . Source: https://i2.wp.com/sourcedexter.com/wp-content/uploads/2017/05/tensorflow-1.gif . Main idea . . Source: https://i.pinimg.com/originals/0a/76/eb/0a76eb3c95c249cdff9449af08ac4efc.png . There are two main parts . finetuning the model | predicting images | In the first part, we will: . finetune a network to classify images of flower | save the trained model | . In the second part, we will: . load the saved model | use it to predict the category of new images of flower | . Existing networks . AlexNet | VGG | ResNet | SqueezeNet | DenseNet | Inception v3 | GoogLeNet | ShuffleNet v2 | MobileNet v2 | ResNeXt | Wide ResNet | MNASNet | . For more details about these models, check out this documentation: https://pytorch.org/docs/stable/torchvision/models.html . Finetune the model . Import resources . # Pretty display for Jupyter notebooks %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; # Import Python libraries import json from collections import OrderedDict import numpy as np import matplotlib.pyplot as plt import os import sys import copy import time from PIL import Image # Import PyTorch libraries import torch from torch import nn from torch import optim import torch.nn.functional as F from torchvision import datasets, transforms, models, utils from torch.optim import lr_scheduler print(&quot;Using Torch ver. &quot;+torch.__version__) . Using Torch ver. 1.4.0 . Load the dataset . Here we use the flower dataset from udacity. . data_dir = &quot;/content/flower_data&quot; !rm -rf &quot;{data_dir}&quot; !wget -cq https://s3.amazonaws.com/content.udacity-data.com/nd089/flower_data.tar.gz !mkdir &quot;{data_dir}&quot; &amp;&amp; tar -xzf flower_data.tar.gz -C &quot;{data_dir}&quot; !rm -rf &quot;/content/flower_data.tar.gz&quot; . Set parameters . TODO: Explain why we need these params . # Stats needed to normalize ImageNet images means = [0.485, 0.456, 0.406] std_devs = [0.229, 0.224, 0.225] input_size = 224 # Other transforms parameters down_size = 256 rotation = 30 # Determine batch size for DataLoaders batch_size = 16 . Define data transformations . We define the function for the training, validation, and testing dataset. . # Define transforms for the training, validation, and testing sets data_transforms = { &#39;train&#39;: transforms.Compose([ transforms.RandomRotation(rotation), transforms.RandomResizedCrop(input_size), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(means,std_devs)]), &#39;valid&#39;: transforms.Compose( [transforms.Resize(down_size), transforms.CenterCrop(input_size), transforms.ToTensor(), transforms.Normalize(means,std_devs)]), &#39;test&#39;: transforms.Compose([ transforms.Resize(down_size), transforms.CenterCrop(input_size), transforms.ToTensor(), transforms.Normalize(means,std_devs)]) } # TODO: Load the datasets with ImageFolder image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in [&#39;train&#39;, &#39;valid&#39;, &#39;test&#39;]} # TODO: Using the image datasets and the transforms, define the dataloaders dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in [&#39;train&#39;, &#39;valid&#39;, &#39;test&#39;]} class_names = image_datasets[&#39;train&#39;].classes dataset_sizes = {x: len(image_datasets[x]) for x in [&#39;train&#39;, &#39;valid&#39;, &#39;test&#39;]} class_names = image_datasets[&#39;train&#39;].classes device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) . Confirm the data set . First, let&#39;s check out the number of dataset we have! . print(dataset_sizes) print(device) . {&#39;train&#39;: 6552, &#39;valid&#39;: 818, &#39;test&#39;: 819} cuda:0 . We define a function to show the images contains in the first batch. Note that the images have been resized to input_size. . def imshow(inp, title=None): &quot;&quot;&quot;Imshow for Tensor.&quot;&quot;&quot; inp = inp.numpy().transpose((1, 2, 0)) mean = np.array(means) std = np.array(std_devs) inp = std * inp + mean inp = np.clip(inp, 0, 1) plt.imshow(inp) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots are updated # Get a batch of training data inputs, classes = next(iter(dataloaders[&#39;train&#39;])) # Make a grid from batch out = utils.make_grid(inputs) imshow(out, title=[class_names[x] for x in classes]) . Load label mapping . At this point, we need a mapping between the numbered label and the actual names of the flowers. . TODO: add more explanation . # Label mapping !wget -P &quot;{data_dir}&quot; https://raw.githubusercontent.com/udacity/aipnd-project/master/cat_to_name.json with open(os.path.join(data_dir, &#39;cat_to_name.json&#39;), &#39;r&#39;) as f: cat_to_name = json.load(f) . --2020-04-05 12:33:05-- https://raw.githubusercontent.com/udacity/aipnd-project/master/cat_to_name.json Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 2218 (2.2K) [text/plain] Saving to: ‘/content/flower_data/cat_to_name.json’ cat_to_name.json 100%[===================&gt;] 2.17K --.-KB/s in 0s 2020-04-05 12:33:05 (63.8 MB/s) - ‘/content/flower_data/cat_to_name.json’ saved [2218/2218] . # Test the data loader images, labels = next(iter(dataloaders[&#39;train&#39;])) images.size() images, labels = next(iter(dataloaders[&#39;train&#39;])) rand_idx = np.random.randint(len(images)) # print(rand_idx) print(&quot;label: {}, class: {}, name: {}&quot;.format(labels[rand_idx].item(), class_names[labels[rand_idx].item()], cat_to_name[class_names[labels[rand_idx].item()]])) . label: 65, class: 66, name: osteospermum . Build new classifier layers . The classifier layers of the pre-trained network must be replaced with a classifier that fits the parameters of our flower-image classification problem. . Before building and replacing the network classifier, I will exclude all pre-trained parameters from gradient computation. . The input in the new classifier must match the output of the pre-trained network (25,088). The output of the classifier will be the set of image categories in the flowers datasets (102). . TODO: add more explanation . . Tip: To find out more about other predefined models from PyTorch, run the following command and refer to this documentation: https://pytorch.org/docs/stable/torchvision/models.html. . dir(models) . Train the classifier and track running performance . The two main tasks in this section are the following: . Train the classifier layers using backpropagation and the pre-trained network to get the features. | Track the loss and accuracy on the validation set to determine the best hyper-parameters. | Train the classifier layers . I start by instantiating the necessary classes for the optimization component of network training. . First, the loss function will be a Negative Log Likelihood (torch.nn.NLLLoss) . Second, the optimization method will be the Adam algorithm (torch.optim.Adam). . # Instantiate loss function loss_function = nn.NLLLoss() # Instantiate optimization algorithm learning_rate = 0.0001 optimizer = optim.Adam(model_conv.classifier.parameters(), lr=learning_rate) # Enable CUDA: use GPUs for model computation if torch.cuda.is_available(): model_conv.to(&#39;cuda&#39;) # Now we define params for the training loop # Epochs: number of iterations over the entire training dataset epochs = 15 # Number of iterations between printing loss and accuracy print_steps = 30 . Now we can define a function for training the model. . Create the optimizer . Now that the model structure is correct, the final step for finetuning and feature extracting is to create an optimizer that only updates the desired parameters. Recall that after loading the pretrained model, but before reshaping, if feature_extract=True we manually set all of the parameter’s .requires_grad attributes to False. Then the reinitialized layer’s parameters have .requires_grad=True by default. So now we know that all parameters that have .requires_grad=True should be optimized. Next, we make a list of such parameters and input this list to the SGD algorithm constructor. . To verify this, check out the printed parameters to learn. When finetuning, this list should be long and include all of the model parameters. However, when feature extracting this list should be short and only include the weights and biases of the reshaped layers. . model_conv = model_conv.to(device) # Gather the parameters to be optimized/updated in this run. If we are # finetuning we will be updating all parameters. However, if we are # doing feature extract method, we will only update the parameters # that we have just initialized, i.e. the parameters with requires_grad # is True. params_to_update = model_conv.parameters() print(&quot;Params to learn:&quot;) if feature_extract: params_to_update = [] for name,param in model_conv.named_parameters(): if param.requires_grad == True: params_to_update.append(param) print(&quot; t&quot;,name) else: for name,param in model_conv.named_parameters(): if param.requires_grad == True: print(&quot; t&quot;,name) criterion = nn.CrossEntropyLoss() # Observe that all parameters are being optimized optimizer_conv = optim.SGD(params_to_update, lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1) . Params to learn: classifier.6.weight classifier.6.bias . Now we are ready to train our model! . model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=3 ) . Epoch 0/2 - train Loss: 3.0470 Acc: 0.3307 valid Loss: 1.8235 Acc: 0.6015 Epoch 1/2 - train Loss: 1.9147 Acc: 0.5449 valid Loss: 1.3099 Acc: 0.6980 Epoch 2/2 - train Loss: 1.5916 Acc: 0.6090 valid Loss: 1.0807 Acc: 0.7494 Training complete in 3m 22s Best val Acc: 0.749389 . Calculate Accuracy . We can also define a function to calculate the accuracy. . def calc_accuracy(mode=&#39;test&#39;): # Initialize validation counters cnt_correct = 0 cnt_total = 0 # no_grad() prevents tracking history (and using memory) with torch.no_grad(): # Iterate over the entire validation dataset for input_images, labels in dataloaders[mode]: # Enable CUDA: use GPUs for model computation if torch.cuda.is_available(): input_images, labels = input_images.to(&#39;cuda&#39;), labels.to(&#39;cuda&#39;) # Make predictions outputs = model_conv(input_images) _, predicted = torch.max(outputs.data, 1) # Count total and correct predictions cnt_total += labels.size(0) cnt_correct += (predicted == labels).sum().item() print(mode + &#39; accuracy ({0:d} images): {1:.1%}&#39; .format(cnt_total, cnt_correct / cnt_total)) . calc_accuracy(&#39;valid&#39;) calc_accuracy(&#39;test&#39;) . valid accuracy (818 images): 74.9% test accuracy (819 images): 70.9% . calc_accuracy(&#39;train&#39;) . train accuracy (6552 images): 72.7% . Visualized model . TODO: add . def visualize_model(model, num_images=6, mode=&#39;test&#39;): was_training = model.training model.eval() images_so_far = 0 fig = plt.figure() with torch.no_grad(): for i, (inputs, labels) in enumerate(dataloaders[mode]): inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) _, preds = torch.max(outputs, 1) for j in range(inputs.size()[0]): images_so_far += 1 ax = plt.subplot(num_images//2, 2, images_so_far) ax.axis(&#39;off&#39;) # ax.axis([0, 224, 0,224]) num_label = class_names[preds[j]] str_label = cat_to_name[num_label] ax.set_title(&#39;{}&#39;.format(str_label)) imshow(inputs.cpu().data[j]) if images_so_far == num_images: model.train(mode=was_training) return model.train(mode=was_training) # out = utils.make_grid(inputs) # imshow(out) . visualize_model(model_conv, mode=&#39;test&#39;) . Save the trained network . Now that the network is trained, I save the model so it can be loaded later to make predictions. . In order to save the model allowing for the option to keep training it later, I also save the necessary training parameters: number of epochs and the optimizer state. . # Save the mapping of the flower labels (1-102) to array indices (0-101) model_conv.class_to_idx = image_datasets[&#39;train&#39;].class_to_idx # Create dictionary with needed components to rebuild model checkpoint = { &#39;model&#39;: model_conv, &#39;epochs&#39;: epochs, &#39;optimizer_state&#39;: optimizer.state_dict, &#39;labels_to_flower_names&#39;: cat_to_name } # Save checkpoint torch.save(checkpoint, &#39;checkpoint_04-05-2020.pth&#39;) . Predict images based on the model . # Stats needed to normalize ImageNet images ImageNet = { &#39;means&#39; : np.array([0.485, 0.456, 0.406]), &#39;std_devs&#39; : np.array([0.229, 0.224, 0.225]), &#39;short_ax_max&#39; : 255, &#39;resize&#39; : 224 } . # Process a PIL image for use in a PyTorch model def process_image(pil_image, image_reqs=ImageNet): &quot;&quot;&quot;Scale, crop, and normalize a PIL image for a PyTorch model. Returns a NumPy array Args: pil_image (PIL image): Input image for a prediction task image_reqs (dict): Image stats needed so image matches model reqs Returns: NumPy array &quot;&quot;&quot; # Resize image so that shortest side is 256 pixels, keeping ratio img_size = pil_image.size ratio = max(img_size) / min(img_size) new_size = [0, 0] short = img_size.index(min(img_size)) long = 1 - short new_size[short] = image_reqs[&#39;short_ax_max&#39;] new_size[long] = int(image_reqs[&#39;short_ax_max&#39;] * ratio) pil_image = pil_image.resize(size=tuple(new_size)) # Crop out the center 224x224 portion of the image gap_x = int((new_size[0] - image_reqs[&#39;resize&#39;]) / 2) gap_y = int((new_size[1] - image_reqs[&#39;resize&#39;]) / 2) crop_box = (gap_x, gap_y, gap_x + image_reqs[&#39;resize&#39;], gap_y + image_reqs[&#39;resize&#39;]) pil_image = pil_image.crop(box=crop_box) # Re-encode image color channels np_image = np.array(pil_image) / 255 # Normalize image accordingly to the same statistics used to train norm_image = (np_image - image_reqs[&#39;means&#39;]) / image_reqs[&#39;std_devs&#39;] # Reorder dimensions of NumPy array so it matches PyTorch&#39;s input norm_image = norm_image.transpose((2, 0, 1)) return norm_image . Deploy trained network for predictions . Now that images are processed to the expected PyTorch format, they can be passed into a function that uses the trained model to make predictions. . A common practice is to predict the top 5, or so, most probable classes (usually called top-$K$). The function below, predict(), takes the image filepath and the trained model — and returns the ordered lists of the most likely classes and their corresponding probabilities. . # Load a flower image and make a prediction of the top-K classes def predict(img_filepath, model, topk=5): &#39;&#39;&#39; Predict the top-K classes of an image using a trained deep learning model. Args: img_filepath (str): Input image for the prediction task model (torchvision.models): Trained deep learning model topk (int): Number of top most likely predictions Returns: top_probs (list) classes (list) &#39;&#39;&#39; # Open PIL image pil_image = Image.open(img_filepath) # Scale, crop, normalize PIL image np_image = process_image(pil_image, image_reqs=ImageNet) # Resize NumPy array to match dataloader output np_image = np.resize(np_image,(1, 3, 224, 224)) # Covert NumPy array to PyTorch tensor img_tensor = torch.from_numpy(np_image).type(torch.FloatTensor) # Enable CUDA: use GPUs for model computation if torch.cuda.is_available(): img_tensor = img_tensor.to(&#39;cuda&#39;) # Run model in evaluation mode model.eval() with torch.no_grad(): outputs = model(img_tensor) # Convert softmax output to probabilities probs = torch.exp(outputs.data) # Find top-k probabilities and indices top_probs, indices = torch.topk(probs, dim=1, k=topk) # Convert PyTorch tensors to lists top_probs, indices = top_probs.to(&#39;cpu&#39;).numpy(), indices.to(&#39;cpu&#39;).numpy() top_probs, indices = top_probs[0].tolist(), indices[0].tolist() # Find the class using the indices (reverse dictionary first) idx_to_class = {idx: class_ for class_, idx in model.class_to_idx.items()} classes = [idx_to_class[i] for i in indices] return top_probs, classes . Sanity check: Make a flower category prediction . In this final section we will check whether the predictions that come out of the image classification tool make sense. . This will not be a comprehensive test, but just a sanity check to screen for obvious bugs and errors. . The main steps in this prediction test are the following: . Choose a flower image, of known category, from the test set. And run the image through the trained network using the predict() function. . | Get the flower names (in words) from the checkpoint dictionary, for the chosen test flower and the predicted top-$K$ classes. . | Write the function imshow(), which takes the output of the process_image() function and re-processes the array to be plotted with matplotlib. This will let us visualize the input image as seen by the trained model. . | Display the test flower image along with a bar chart of the top-5 predicted categories. . | Choose test image and run through the model. . # Choose a flower category and individual image for prediction example test_class = &#39;74&#39; pathname = data_dir+&#39;/test/&#39;+test_class+&quot;/&quot; import random rand_file = random.choice(os.listdir(pathname)) filepath = pathname+rand_file # print(filepath) # Run predict() to get model predictions: top-k classes and probabilities probs, classes = predict(filepath, model_conv, topk=5) . Get flower names from checkpoint dictionary . # Get the flower-category names from the saved checkpoint flower_names = checkpoint[&#39;labels_to_flower_names&#39;] test_flower_name = flower_names[test_class] top_k_names = [flower_names[key] for key in classes] . def imshow(image, ax=None, title=None, image_reqs=ImageNet): if ax is None: fig, ax = plt.subplots() # PyTorch tensors assume the color channel is the first dimension # but matplotlib assumes is the third dimension image = image.transpose((1, 2, 0)) # Undo preprocessing mean = image_reqs[&#39;means&#39;] std = image_reqs[&#39;std_devs&#39;] image = std * image + mean # Image needs to be clipped between 0 and 1 image = np.clip(image, 0, 1) ax.imshow(image) . # Create figure and axes fig, (flower, prob_bars) = plt.subplots(nrows=2, ncols=1, figsize=(6,8)) # Render the flower image imshow(image=process_image(Image.open(filepath)), ax=flower) flower.set_title(&#39;Test flower category: n— {} —&#39;.format(test_flower_name), fontsize=16) # Make barchart of top-k probabilities index = np.arange(len(classes))[::-1] prob_bars.barh(index, probs, tick_label=top_k_names, align=&#39;center&#39;) prob_bars.set_title(&#39;Model prediction&#39;, fontsize=14) prob_bars.set_xlabel(&#39;Probability&#39;, fontsize=12) prob_bars.set_ylabel(&#39;Top-K classes&#39;, fontsize=12) # Plot the final figure fig.tight_layout() . Conclusion . TODO: add conclusion . Reference . This notebook is a compilation of multiple information taken from the following different sources. Credits go to their respective authors. . TODO: add more reference . https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html | https://cs231n.github.io/transfer-learning/ | https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html | https://medium.com/datadriveninvestor/creating-a-pytorch-image-classifier-da9db139ba80 | https://github.com/jclh/image-classifier-PyTorch/blob/master/flower-classifier-PyTorch.ipynb | .",
            "url": "https://noxouille.github.io/pydata2020-3LVDL/image%20classification/2020/04/16/level1.html",
            "relUrl": "/image%20classification/2020/04/16/level1.html",
            "date": " • Apr 16, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://noxouille.github.io/pydata2020-3LVDL/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://noxouille.github.io/pydata2020-3LVDL/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}