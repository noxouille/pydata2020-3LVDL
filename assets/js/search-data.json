{
  
    
        "post0": {
            "title": "Level 1 - Image Classification",
            "content": "Compiled by: Ferdi Pratama . GPU check . First, simply select &quot;GPU&quot; in the Accelerator drop-down in Notebook Settings (either through the Edit menu or the command palette at cmd/ctrl-shift-P). . Then, we should test if the GPU is detected, and find out which GPU we are using. . !nvidia-smi . Sun Apr 5 14:09:07 2020 +--+ | NVIDIA-SMI 440.64.00 Driver Version: 418.67 CUDA Version: 10.1 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 | | N/A 34C P0 26W / 250W | 0MiB / 16280MiB | 0% Default | +-+-+-+ +--+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +--+ . Overview . . Source: https://i.pinimg.com/originals/0a/76/eb/0a76eb3c95c249cdff9449af08ac4efc.png . There are two main parts . finetuning the model | predicting images | In the first part, we will: . finetune a network to classify images of flower | save the trained model | . In the second part, we will: . load the saved model | use it to predict the category of new images of flower | . Finetune the model . Import resources . # Pretty display for Jupyter notebooks %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; # Import Python libraries import json from collections import OrderedDict import numpy as np import matplotlib.pyplot as plt import os import sys import copy import time from PIL import Image # Import PyTorch libraries import torch from torch import nn from torch import optim import torch.nn.functional as F from torchvision import datasets, transforms, models, utils from torch.optim import lr_scheduler print(&quot;Using Torch ver. &quot;+torch.__version__) . Using Torch ver. 1.4.0 . Load the dataset . Here we use the flower dataset from udacity. . data_dir = &quot;/content/flower_data&quot; !rm -rf &quot;{data_dir}&quot; !wget -cq https://s3.amazonaws.com/content.udacity-data.com/nd089/flower_data.tar.gz !mkdir &quot;{data_dir}&quot; &amp;&amp; tar -xzf flower_data.tar.gz -C &quot;{data_dir}&quot; !rm -rf &quot;/content/flower_data.tar.gz&quot; . Set parameters . TODO: Explain why we need these params . # Stats needed to normalize ImageNet images means = [0.485, 0.456, 0.406] std_devs = [0.229, 0.224, 0.225] input_size = 224 # Other transforms parameters down_size = 256 rotation = 30 # Determine batch size for DataLoaders batch_size = 16 . Define data transformations . We define the function for the training, validation, and testing dataset. . # Define transforms for the training, validation, and testing sets data_transforms = { &#39;train&#39;: transforms.Compose([ transforms.RandomRotation(rotation), transforms.RandomResizedCrop(input_size), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(means,std_devs)]), &#39;valid&#39;: transforms.Compose( [transforms.Resize(down_size), transforms.CenterCrop(input_size), transforms.ToTensor(), transforms.Normalize(means,std_devs)]), &#39;test&#39;: transforms.Compose([ transforms.Resize(down_size), transforms.CenterCrop(input_size), transforms.ToTensor(), transforms.Normalize(means,std_devs)]) } # TODO: Load the datasets with ImageFolder image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in [&#39;train&#39;, &#39;valid&#39;, &#39;test&#39;]} # TODO: Using the image datasets and the transforms, define the dataloaders dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in [&#39;train&#39;, &#39;valid&#39;, &#39;test&#39;]} class_names = image_datasets[&#39;train&#39;].classes dataset_sizes = {x: len(image_datasets[x]) for x in [&#39;train&#39;, &#39;valid&#39;, &#39;test&#39;]} class_names = image_datasets[&#39;train&#39;].classes device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) . Confirm the data set . First, let&#39;s check out the number of dataset we have! . print(dataset_sizes) print(device) . {&#39;train&#39;: 6552, &#39;valid&#39;: 818, &#39;test&#39;: 819} cuda:0 . We define a function to show the images contains in the first batch. Note that the images have been resized to input_size. . def imshow(inp, title=None): &quot;&quot;&quot;Imshow for Tensor.&quot;&quot;&quot; inp = inp.numpy().transpose((1, 2, 0)) mean = np.array(means) std = np.array(std_devs) inp = std * inp + mean inp = np.clip(inp, 0, 1) plt.imshow(inp) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots are updated # Get a batch of training data inputs, classes = next(iter(dataloaders[&#39;train&#39;])) # Make a grid from batch out = utils.make_grid(inputs) imshow(out, title=[class_names[x] for x in classes]) . Load label mapping . At this point, we need a mapping between the numbered label and the actual names of the flowers. . TODO: add more explanation . # Label mapping !wget -P &quot;{data_dir}&quot; https://raw.githubusercontent.com/udacity/aipnd-project/master/cat_to_name.json with open(os.path.join(data_dir, &#39;cat_to_name.json&#39;), &#39;r&#39;) as f: cat_to_name = json.load(f) . --2020-04-05 12:33:05-- https://raw.githubusercontent.com/udacity/aipnd-project/master/cat_to_name.json Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 2218 (2.2K) [text/plain] Saving to: ‘/content/flower_data/cat_to_name.json’ cat_to_name.json 100%[===================&gt;] 2.17K --.-KB/s in 0s 2020-04-05 12:33:05 (63.8 MB/s) - ‘/content/flower_data/cat_to_name.json’ saved [2218/2218] . # Test the data loader images, labels = next(iter(dataloaders[&#39;train&#39;])) images.size() images, labels = next(iter(dataloaders[&#39;train&#39;])) rand_idx = np.random.randint(len(images)) # print(rand_idx) print(&quot;label: {}, class: {}, name: {}&quot;.format(labels[rand_idx].item(), class_names[labels[rand_idx].item()], cat_to_name[class_names[labels[rand_idx].item()]])) . label: 65, class: 66, name: osteospermum . Build new classifier layers . The classifier layers of the pre-trained network must be replaced with a classifier that fits the parameters of our flower-image classification problem. . Before building and replacing the network classifier, I will exclude all pre-trained parameters from gradient computation. . The input in the new classifier must match the output of the pre-trained network (25,088). The output of the classifier will be the set of image categories in the flowers datasets (102). . TODO: add more explanation . def set_parameter_requires_grad(model, feature_extracting): # Freeze updating of pre-trained parameters # I decided to make sure I only trained the classifier parameters here # while having feature parameters frozen. if feature_extracting: for param in model.parameters(): param.requires_grad = False def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True): # Initialize these variables which will be set in this if statement. # Each of these variables is model specific. model_ft = None input_size = 0 if model_name == &quot;resnet&quot;: &quot;&quot;&quot; Resnet18 &quot;&quot;&quot; model_ft = models.resnet18(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) num_ftrs = model_ft.fc.in_features model_ft.fc = nn.Linear(num_ftrs, num_classes) input_size = 224 elif model_name == &quot;alexnet&quot;: &quot;&quot;&quot; Alexnet &quot;&quot;&quot; model_ft = models.alexnet(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) num_ftrs = model_ft.classifier[6].in_features model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes) input_size = 224 elif model_name == &quot;vgg19&quot;: &quot;&quot;&quot; VGG19 &quot;&quot;&quot; model_ft = models.vgg19(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) num_ftrs = model_ft.classifier[6].in_features model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes) input_size = 224 elif model_name == &quot;vgg11&quot;: &quot;&quot;&quot; VGG11_bn &quot;&quot;&quot; model_ft = models.vgg11_bn(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) num_ftrs = model_ft.classifier[6].in_features model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes) input_size = 224 elif model_name == &quot;squeezenet&quot;: &quot;&quot;&quot; Squeezenet &quot;&quot;&quot; model_ft = models.squeezenet1_0(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1)) model_ft.num_classes = num_classes input_size = 224 elif model_name == &quot;densenet&quot;: &quot;&quot;&quot; Densenet &quot;&quot;&quot; model_ft = models.densenet121(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) num_ftrs = model_ft.classifier.in_features model_ft.classifier = nn.Linear(num_ftrs, num_classes) input_size = 224 elif model_name == &quot;inception&quot;: &quot;&quot;&quot; Inception v3 Be careful, expects (299,299) sized images and has auxiliary output &quot;&quot;&quot; model_ft = models.inception_v3(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) # Handle the auxilary net num_ftrs = model_ft.AuxLogits.fc.in_features model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes) # Handle the primary net num_ftrs = model_ft.fc.in_features model_ft.fc = nn.Linear(num_ftrs,num_classes) input_size = 299 else: print(&quot;Invalid model name, exiting...&quot;) exit() return model_ft, input_size model_name = &#39;vgg19&#39; num_classes = len(class_names) feature_extract = True # Initialize the model for this run model_conv, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True) # Print the model we just instantiated print(model_conv) . Downloading: &#34;https://download.pytorch.org/models/vgg19-dcbb9e9d.pth&#34; to /root/.cache/torch/checkpoints/vgg19-dcbb9e9d.pth . VGG( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace=True) (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU(inplace=True) (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (6): ReLU(inplace=True) (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (8): ReLU(inplace=True) (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace=True) (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (13): ReLU(inplace=True) (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (15): ReLU(inplace=True) (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (17): ReLU(inplace=True) (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (20): ReLU(inplace=True) (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (22): ReLU(inplace=True) (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (24): ReLU(inplace=True) (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (26): ReLU(inplace=True) (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (29): ReLU(inplace=True) (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (31): ReLU(inplace=True) (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (33): ReLU(inplace=True) (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (35): ReLU(inplace=True) (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (avgpool): AdaptiveAvgPool2d(output_size=(7, 7)) (classifier): Sequential( (0): Linear(in_features=25088, out_features=4096, bias=True) (1): ReLU(inplace=True) (2): Dropout(p=0.5, inplace=False) (3): Linear(in_features=4096, out_features=4096, bias=True) (4): ReLU(inplace=True) (5): Dropout(p=0.5, inplace=False) (6): Linear(in_features=4096, out_features=102, bias=True) ) ) . Tips . To find out more about other predefined models from PyTorch, run the following command and refer to this documentation: . https://pytorch.org/docs/stable/torchvision/models.html. . dir(models) . Train the classifier and track running performance . The two main tasks in this section are the following: . Train the classifier layers using backpropagation and the pre-trained network to get the features. | Track the loss and accuracy on the validation set to determine the best hyper-parameters. | Train the classifier layers . I start by instantiating the necessary classes for the optimization component of network training. . First, the loss function will be a Negative Log Likelihood (torch.nn.NLLLoss) . Second, the optimization method will be the Adam algorithm (torch.optim.Adam). . # Instantiate loss function loss_function = nn.NLLLoss() # Instantiate optimization algorithm learning_rate = 0.0001 optimizer = optim.Adam(model_conv.classifier.parameters(), lr=learning_rate) # Enable CUDA: use GPUs for model computation if torch.cuda.is_available(): model_conv.to(&#39;cuda&#39;) # Now we define params for the training loop # Epochs: number of iterations over the entire training dataset epochs = 15 # Number of iterations between printing loss and accuracy print_steps = 30 . Now we can define a function for training the model. . def train_model(model, criterion, optimizer, scheduler, num_epochs=25): since = time.time() best_model_wts = copy.deepcopy(model.state_dict()) best_acc = 0.0 for epoch in range(num_epochs): print(&#39;Epoch {}/{}&#39;.format(epoch, num_epochs - 1)) print(&#39;-&#39; * 10) # Each epoch has a training and validation phase for phase in [&#39;train&#39;, &#39;valid&#39;]: if phase == &#39;train&#39;: model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward # track history if only in train with torch.set_grad_enabled(phase == &#39;train&#39;): outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) # backward + optimize only if in training phase if phase == &#39;train&#39;: loss.backward() optimizer.step() # statistics running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) if phase == &#39;train&#39;: scheduler.step() epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects.double() / dataset_sizes[phase] print(&#39;{} Loss: {:.4f} Acc: {:.4f}&#39;.format( phase, epoch_loss, epoch_acc)) # deep copy the model if phase == &#39;valid&#39; and epoch_acc &gt; best_acc: best_acc = epoch_acc best_model_wts = copy.deepcopy(model.state_dict()) print() time_elapsed = time.time() - since print(&#39;Training complete in {:.0f}m {:.0f}s&#39;.format( time_elapsed // 60, time_elapsed % 60)) print(&#39;Best val Acc: {:4f}&#39;.format(best_acc)) # load best model weights model.load_state_dict(best_model_wts) return model . Create the optimizer . Now that the model structure is correct, the final step for finetuning and feature extracting is to create an optimizer that only updates the desired parameters. Recall that after loading the pretrained model, but before reshaping, if feature_extract=True we manually set all of the parameter’s .requires_grad attributes to False. Then the reinitialized layer’s parameters have .requires_grad=True by default. So now we know that all parameters that have .requires_grad=True should be optimized. Next, we make a list of such parameters and input this list to the SGD algorithm constructor. . To verify this, check out the printed parameters to learn. When finetuning, this list should be long and include all of the model parameters. However, when feature extracting this list should be short and only include the weights and biases of the reshaped layers. . model_conv = model_conv.to(device) # Gather the parameters to be optimized/updated in this run. If we are # finetuning we will be updating all parameters. However, if we are # doing feature extract method, we will only update the parameters # that we have just initialized, i.e. the parameters with requires_grad # is True. params_to_update = model_conv.parameters() print(&quot;Params to learn:&quot;) if feature_extract: params_to_update = [] for name,param in model_conv.named_parameters(): if param.requires_grad == True: params_to_update.append(param) print(&quot; t&quot;,name) else: for name,param in model_conv.named_parameters(): if param.requires_grad == True: print(&quot; t&quot;,name) criterion = nn.CrossEntropyLoss() # Observe that all parameters are being optimized optimizer_conv = optim.SGD(params_to_update, lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1) . Params to learn: classifier.6.weight classifier.6.bias . Now we are ready to train our model! . model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=3 ) . Epoch 0/2 - train Loss: 3.0470 Acc: 0.3307 valid Loss: 1.8235 Acc: 0.6015 Epoch 1/2 - train Loss: 1.9147 Acc: 0.5449 valid Loss: 1.3099 Acc: 0.6980 Epoch 2/2 - train Loss: 1.5916 Acc: 0.6090 valid Loss: 1.0807 Acc: 0.7494 Training complete in 3m 22s Best val Acc: 0.749389 . Calculate Accuracy . We can also define a function to calculate the accuracy. . def calc_accuracy(mode=&#39;test&#39;): # Initialize validation counters cnt_correct = 0 cnt_total = 0 # no_grad() prevents tracking history (and using memory) with torch.no_grad(): # Iterate over the entire validation dataset for input_images, labels in dataloaders[mode]: # Enable CUDA: use GPUs for model computation if torch.cuda.is_available(): input_images, labels = input_images.to(&#39;cuda&#39;), labels.to(&#39;cuda&#39;) # Make predictions outputs = model_conv(input_images) _, predicted = torch.max(outputs.data, 1) # Count total and correct predictions cnt_total += labels.size(0) cnt_correct += (predicted == labels).sum().item() print(mode + &#39; accuracy ({0:d} images): {1:.1%}&#39; .format(cnt_total, cnt_correct / cnt_total)) . calc_accuracy(&#39;valid&#39;) calc_accuracy(&#39;test&#39;) . valid accuracy (818 images): 74.9% test accuracy (819 images): 70.9% . calc_accuracy(&#39;train&#39;) . train accuracy (6552 images): 72.7% . Visualized model . TODO: add . def visualize_model(model, num_images=6, mode=&#39;test&#39;): was_training = model.training model.eval() images_so_far = 0 fig = plt.figure() with torch.no_grad(): for i, (inputs, labels) in enumerate(dataloaders[mode]): inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) _, preds = torch.max(outputs, 1) for j in range(inputs.size()[0]): images_so_far += 1 ax = plt.subplot(num_images//2, 2, images_so_far) ax.axis(&#39;off&#39;) # ax.axis([0, 224, 0,224]) num_label = class_names[preds[j]] str_label = cat_to_name[num_label] ax.set_title(&#39;{}&#39;.format(str_label)) imshow(inputs.cpu().data[j]) if images_so_far == num_images: model.train(mode=was_training) return model.train(mode=was_training) # out = utils.make_grid(inputs) # imshow(out) . visualize_model(model_conv, mode=&#39;test&#39;) . Save the trained network . Now that the network is trained, I save the model so it can be loaded later to make predictions. . In order to save the model allowing for the option to keep training it later, I also save the necessary training parameters: number of epochs and the optimizer state. . # Save the mapping of the flower labels (1-102) to array indices (0-101) model_conv.class_to_idx = image_datasets[&#39;train&#39;].class_to_idx # Create dictionary with needed components to rebuild model checkpoint = { &#39;model&#39;: model_conv, &#39;epochs&#39;: epochs, &#39;optimizer_state&#39;: optimizer.state_dict, &#39;labels_to_flower_names&#39;: cat_to_name } # Save checkpoint torch.save(checkpoint, &#39;checkpoint_04-05-2020.pth&#39;) . Predict images based on the model . # Stats needed to normalize ImageNet images ImageNet = { &#39;means&#39; : np.array([0.485, 0.456, 0.406]), &#39;std_devs&#39; : np.array([0.229, 0.224, 0.225]), &#39;short_ax_max&#39; : 255, &#39;resize&#39; : 224 } . # Process a PIL image for use in a PyTorch model def process_image(pil_image, image_reqs=ImageNet): &quot;&quot;&quot;Scale, crop, and normalize a PIL image for a PyTorch model. Returns a NumPy array Args: pil_image (PIL image): Input image for a prediction task image_reqs (dict): Image stats needed so image matches model reqs Returns: NumPy array &quot;&quot;&quot; # Resize image so that shortest side is 256 pixels, keeping ratio img_size = pil_image.size ratio = max(img_size) / min(img_size) new_size = [0, 0] short = img_size.index(min(img_size)) long = 1 - short new_size[short] = image_reqs[&#39;short_ax_max&#39;] new_size[long] = int(image_reqs[&#39;short_ax_max&#39;] * ratio) pil_image = pil_image.resize(size=tuple(new_size)) # Crop out the center 224x224 portion of the image gap_x = int((new_size[0] - image_reqs[&#39;resize&#39;]) / 2) gap_y = int((new_size[1] - image_reqs[&#39;resize&#39;]) / 2) crop_box = (gap_x, gap_y, gap_x + image_reqs[&#39;resize&#39;], gap_y + image_reqs[&#39;resize&#39;]) pil_image = pil_image.crop(box=crop_box) # Re-encode image color channels np_image = np.array(pil_image) / 255 # Normalize image accordingly to the same statistics used to train norm_image = (np_image - image_reqs[&#39;means&#39;]) / image_reqs[&#39;std_devs&#39;] # Reorder dimensions of NumPy array so it matches PyTorch&#39;s input norm_image = norm_image.transpose((2, 0, 1)) return norm_image . Deploy trained network for predictions . Now that images are processed to the expected PyTorch format, they can be passed into a function that uses the trained model to make predictions. . A common practice is to predict the top 5, or so, most probable classes (usually called top-$K$). The function below, predict(), takes the image filepath and the trained model — and returns the ordered lists of the most likely classes and their corresponding probabilities. . # Load a flower image and make a prediction of the top-K classes def predict(img_filepath, model, topk=5): &#39;&#39;&#39; Predict the top-K classes of an image using a trained deep learning model. Args: img_filepath (str): Input image for the prediction task model (torchvision.models): Trained deep learning model topk (int): Number of top most likely predictions Returns: top_probs (list) classes (list) &#39;&#39;&#39; # Open PIL image pil_image = Image.open(img_filepath) # Scale, crop, normalize PIL image np_image = process_image(pil_image, image_reqs=ImageNet) # Resize NumPy array to match dataloader output np_image = np.resize(np_image,(1, 3, 224, 224)) # Covert NumPy array to PyTorch tensor img_tensor = torch.from_numpy(np_image).type(torch.FloatTensor) # Enable CUDA: use GPUs for model computation if torch.cuda.is_available(): img_tensor = img_tensor.to(&#39;cuda&#39;) # Run model in evaluation mode model.eval() with torch.no_grad(): outputs = model(img_tensor) # Convert softmax output to probabilities probs = torch.exp(outputs.data) # Find top-k probabilities and indices top_probs, indices = torch.topk(probs, dim=1, k=topk) # Convert PyTorch tensors to lists top_probs, indices = top_probs.to(&#39;cpu&#39;).numpy(), indices.to(&#39;cpu&#39;).numpy() top_probs, indices = top_probs[0].tolist(), indices[0].tolist() # Find the class using the indices (reverse dictionary first) idx_to_class = {idx: class_ for class_, idx in model.class_to_idx.items()} classes = [idx_to_class[i] for i in indices] return top_probs, classes . Sanity check: Make a flower category prediction . In this final section we will check whether the predictions that come out of the image classification tool make sense. . This will not be a comprehensive test, but just a sanity check to screen for obvious bugs and errors. . The main steps in this prediction test are the following: . Choose a flower image, of known category, from the test set. And run the image through the trained network using the predict() function. . | Get the flower names (in words) from the checkpoint dictionary, for the chosen test flower and the predicted top-$K$ classes. . | Write the function imshow(), which takes the output of the process_image() function and re-processes the array to be plotted with matplotlib. This will let us visualize the input image as seen by the trained model. . | Display the test flower image along with a bar chart of the top-5 predicted categories. . | Choose test image and run through the model. . # Choose a flower category and individual image for prediction example test_class = &#39;74&#39; pathname = data_dir+&#39;/test/&#39;+test_class+&quot;/&quot; import random rand_file = random.choice(os.listdir(pathname)) filepath = pathname+rand_file # print(filepath) # Run predict() to get model predictions: top-k classes and probabilities probs, classes = predict(filepath, model_conv, topk=5) . Get flower names from checkpoint dictionary . # Get the flower-category names from the saved checkpoint flower_names = checkpoint[&#39;labels_to_flower_names&#39;] test_flower_name = flower_names[test_class] top_k_names = [flower_names[key] for key in classes] . def imshow(image, ax=None, title=None, image_reqs=ImageNet): if ax is None: fig, ax = plt.subplots() # PyTorch tensors assume the color channel is the first dimension # but matplotlib assumes is the third dimension image = image.transpose((1, 2, 0)) # Undo preprocessing mean = image_reqs[&#39;means&#39;] std = image_reqs[&#39;std_devs&#39;] image = std * image + mean # Image needs to be clipped between 0 and 1 image = np.clip(image, 0, 1) ax.imshow(image) . # Create figure and axes fig, (flower, prob_bars) = plt.subplots(nrows=2, ncols=1, figsize=(6,8)) # Render the flower image imshow(image=process_image(Image.open(filepath)), ax=flower) flower.set_title(&#39;Test flower category: n— {} —&#39;.format(test_flower_name), fontsize=16) # Make barchart of top-k probabilities index = np.arange(len(classes))[::-1] prob_bars.barh(index, probs, tick_label=top_k_names, align=&#39;center&#39;) prob_bars.set_title(&#39;Model prediction&#39;, fontsize=14) prob_bars.set_xlabel(&#39;Probability&#39;, fontsize=12) prob_bars.set_ylabel(&#39;Top-K classes&#39;, fontsize=12) # Plot the final figure fig.tight_layout() . Conclusion . TODO: add conclusion . Reference . This notebook is a compilation of multiple information taken from the following different sources. Credits go to their respective authors. . TODO: add more reference . https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html | https://cs231n.github.io/transfer-learning/ | https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html | https://medium.com/datadriveninvestor/creating-a-pytorch-image-classifier-da9db139ba80 | https://github.com/jclh/image-classifier-PyTorch/blob/master/flower-classifier-PyTorch.ipynb | .",
            "url": "https://noxouille.github.io/pydata2020-3LVDL/image%20classification/2020/04/16/level1.html",
            "relUrl": "/image%20classification/2020/04/16/level1.html",
            "date": " • Apr 16, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Level 2 - Object Detection",
            "content": "Compiled by: Ferdi Pratama . Overview . The command below will load an SSD model pretrained on COCO dataset from Torch Hub. . Setting precision = fp16 will load a checkpoint trained with mixed precision into architecture enabling execution on Tensor Cores. Handling mixed precision data requires Apex library. . import torch precision = &#39;fp32&#39; ssd_model = torch.hub.load(&#39;NVIDIA/DeepLearningExamples:torchhub&#39;, &#39;nvidia_ssd&#39;, model_math=precision) . Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub . Torch Hub . TODO: explain torch hub . For convenient and comprehensive formatting of input and output of the model, load a set of utility methods. . utils = torch.hub.load(&#39;NVIDIA/DeepLearningExamples:torchhub&#39;, &#39;nvidia_ssd_processing_utils&#39;) . Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub . Prepare the model for inference . ssd_model.to(&#39;cuda&#39;) ssd_model.eval() . SSD300( (feature_extractor): ResNet( (feature_extractor): Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): Bottleneck( (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (5): Sequential( (0): Bottleneck( (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (6): Sequential( (0): Bottleneck( (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (4): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (5): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) ) ) (additional_blocks): ModuleList( (0): Sequential( (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (1): Sequential( (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (2): Sequential( (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (3): Sequential( (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), bias=False) (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (4): Sequential( (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), bias=False) (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) ) (loc): ModuleList( (0): Conv2d(1024, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (5): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) ) (conf): ModuleList( (0): Conv2d(1024, 324, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): Conv2d(512, 486, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (2): Conv2d(512, 486, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): Conv2d(256, 486, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): Conv2d(256, 324, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (5): Conv2d(256, 324, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) ) ) . Define test images . Below, we add some URLs of images that we want to test. . uris = [ &#39;https://c8.alamy.com/comp/X2ND8M/couple-eating-pizza-in-living-room-X2ND8M.jpg&#39;, &#39;https://www.saltwire.com/media/photologue/photos/cache/RisingSeas-story7sidebar-fcampbellHeavyTrafficHfx2678_large.jpg&#39;, &#39;https://www.reynoldskitchens.com/sites/default/files/styles/hero_image/public/recipes/broccoli-carrots-with-oranges_1.jpg?itok=0ZbTVFho&#39; ] . Format the images to comply with the network input and convert them to tensor. . inputs = [utils.prepare_input(uri) for uri in uris] tensor = utils.prepare_tensor(inputs, precision == &#39;fp16&#39;) . Run the SSD network . Finally, we can perform object detection. . with torch.no_grad(): detections_batch = ssd_model(tensor) . By default, raw output from SSD network per input image contains 8732 boxes with localization and class probability distribution. Let&#39;s filter this output to only get reasonable detections (confidence&gt;40%) in a more comprehensive format. . results_per_input = utils.decode_results(detections_batch) best_results_per_input = [utils.pick_best(results, 0.40) for results in results_per_input] . The model was trained on COCO dataset, which we need to access in order to translate class IDs into object names. For the first time, downloading annotations may take a while. . classes_to_labels = utils.get_coco_object_dictionary() . Check out the label definitions . #collapse-hide print(&#39; n&#39;.join(classes_to_labels)) . . person bicycle car motorcycle airplane bus train truck boat traffic light fire hydrant stop sign parking meter bench bird cat dog horse sheep cow elephant bear zebra giraffe backpack umbrella handbag tie suitcase frisbee skis snowboard sports ball kite baseball bat baseball glove skateboard surfboard tennis racket bottle wine glass cup fork knife spoon bowl banana apple sandwich orange broccoli carrot hot dog pizza donut cake chair couch potted plant bed dining table toilet tv laptop mouse remote keyboard cell phone microwave oven toaster sink refrigerator book clock vase scissors teddy bear hair drier toothbrush . Visualize the results . First, we generate random colors, so that we get different color for every class label. . import colorsys import random def random_colors(N, bright=True): &quot;&quot;&quot; Generate random colors. To get visually distinct colors, generate them in HSV space then convert to RGB. &quot;&quot;&quot; brightness = 1.0 if bright else 0.7 hsv = [(i / N, 1, brightness) for i in range(N)] colors = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv)) random.shuffle(colors) return colors colors = random_colors(len(classes_to_labels)) . from matplotlib import pyplot as plt import matplotlib.patches as patches fig, axs = plt.subplots(1, 3, figsize=(10, 4), dpi=150) for ax, image_idx in zip(axs, range(len(best_results_per_input))): # Show original, denormalized image... image = inputs[image_idx] / 2 + 0.5 ax.imshow(image) ax.axis(&#39;off&#39;) # ...with detections bboxes, classes, confidences = best_results_per_input[image_idx] for idx in range(len(bboxes)): left, bot, right, top = bboxes[idx] x, y, w, h = [val * 300 for val in [left, bot, right - left, top - bot]] rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor=colors[classes[idx]], facecolor=&#39;none&#39;) ax.add_patch(rect) ax.text(x, y, &quot;{} {:.0f}%&quot;.format(classes_to_labels[classes[idx] - 1], confidences[idx]*100), bbox=dict(facecolor=colors[classes[idx]], alpha=0.4), fontsize=13) plt.show() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . Conclusion . TODO: add conclusion . References . https://pytorch.org/hub/nvidia_deeplearningexamples_ssd/ https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD | https://towardsdatascience.com/object-detection-and-tracking-in-pytorch-b3cf1a696a98 | .",
            "url": "https://noxouille.github.io/pydata2020-3LVDL/object%20detection/2020/04/15/level2.html",
            "relUrl": "/object%20detection/2020/04/15/level2.html",
            "date": " • Apr 15, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://noxouille.github.io/pydata2020-3LVDL/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://noxouille.github.io/pydata2020-3LVDL/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://noxouille.github.io/pydata2020-3LVDL/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://noxouille.github.io/pydata2020-3LVDL/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}