{
  
    
        "post0": {
            "title": "Level 1 - Image Classification",
            "content": "Compiled by: Ferdi Pratama . GPU check . First, simply select &quot;GPU&quot; in the Accelerator drop-down in Notebook Settings (either through the Edit menu or the command palette at cmd/ctrl-shift-P). . Then, we should test if the GPU is detected, and find out which GPU we are using. . !nvidia-smi . Sun Apr 5 14:09:07 2020 +--+ | NVIDIA-SMI 440.64.00 Driver Version: 418.67 CUDA Version: 10.1 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 | | N/A 34C P0 26W / 250W | 0MiB / 16280MiB | 0% Default | +-+-+-+ +--+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +--+ . Overview . . Source: https://i2.wp.com/sourcedexter.com/wp-content/uploads/2017/05/tensorflow-1.gif . Main idea . . Source: https://i.pinimg.com/originals/0a/76/eb/0a76eb3c95c249cdff9449af08ac4efc.png . There are two main parts . finetuning the model | predicting images | In the first part, we will: . finetune a network to classify images of flower | save the trained model | . In the second part, we will: . load the saved model | use it to predict the category of new images of flower | . Existing networks . AlexNet | VGG | ResNet | SqueezeNet | DenseNet | Inception v3 | GoogLeNet | ShuffleNet v2 | MobileNet v2 | ResNeXt | Wide ResNet | MNASNet | . For more details about these models, check out this documentation: https://pytorch.org/docs/stable/torchvision/models.html . Finetune the model . Import resources . # Pretty display for Jupyter notebooks %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; # Import Python libraries import json from collections import OrderedDict import numpy as np import matplotlib.pyplot as plt import os import sys import copy import time from PIL import Image # Import PyTorch libraries import torch from torch import nn from torch import optim import torch.nn.functional as F from torchvision import datasets, transforms, models, utils from torch.optim import lr_scheduler print(&quot;Using Torch ver. &quot;+torch.__version__) . Using Torch ver. 1.4.0 . Load the dataset . Here we use the flower dataset from udacity. . data_dir = &quot;/content/flower_data&quot; !rm -rf &quot;{data_dir}&quot; !wget -cq https://s3.amazonaws.com/content.udacity-data.com/nd089/flower_data.tar.gz !mkdir &quot;{data_dir}&quot; &amp;&amp; tar -xzf flower_data.tar.gz -C &quot;{data_dir}&quot; !rm -rf &quot;/content/flower_data.tar.gz&quot; . Set parameters . It is a good practice to make sure that we minimize the risk of vanishing gradient, by normalizing the input image, in terms of the means and std devs. You can find a good explanation here. . # Stats needed to normalize ImageNet images means = [0.485, 0.456, 0.406] std_devs = [0.229, 0.224, 0.225] input_size = 224 # Other transforms parameters down_size = 256 rotation = 30 # Determine batch size for DataLoaders batch_size = 16 . Define data transformations . We define the function for the training, validation, and testing dataset. . # Define transforms for the training, validation, and testing sets data_transforms = { &#39;train&#39;: transforms.Compose([ transforms.RandomRotation(rotation), transforms.RandomResizedCrop(input_size), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(means,std_devs)]), &#39;valid&#39;: transforms.Compose( [transforms.Resize(down_size), transforms.CenterCrop(input_size), transforms.ToTensor(), transforms.Normalize(means,std_devs)]), &#39;test&#39;: transforms.Compose([ transforms.Resize(down_size), transforms.CenterCrop(input_size), transforms.ToTensor(), transforms.Normalize(means,std_devs)]) } # TODO: Load the datasets with ImageFolder image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in [&#39;train&#39;, &#39;valid&#39;, &#39;test&#39;]} # TODO: Using the image datasets and the transforms, define the dataloaders dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in [&#39;train&#39;, &#39;valid&#39;, &#39;test&#39;]} class_names = image_datasets[&#39;train&#39;].classes dataset_sizes = {x: len(image_datasets[x]) for x in [&#39;train&#39;, &#39;valid&#39;, &#39;test&#39;]} class_names = image_datasets[&#39;train&#39;].classes device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) . Confirm the data set . First, let&#39;s check out the number of dataset we have! . print(dataset_sizes) print(device) . {&#39;train&#39;: 6552, &#39;valid&#39;: 818, &#39;test&#39;: 819} cuda:0 . We define a function to show the images contains in the first batch. Note that the images have been resized to input_size. . def imshow(inp, title=None): &quot;&quot;&quot;Imshow for Tensor.&quot;&quot;&quot; inp = inp.numpy().transpose((1, 2, 0)) mean = np.array(means) std = np.array(std_devs) inp = std * inp + mean inp = np.clip(inp, 0, 1) plt.imshow(inp) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots are updated # Get a batch of training data inputs, classes = next(iter(dataloaders[&#39;train&#39;])) # Make a grid from batch out = utils.make_grid(inputs) imshow(out, title=[class_names[x] for x in classes]) . Load label mapping . At this point, we need a mapping between the numbered label and the actual names of the flowers. . # Label mapping !wget -P &quot;{data_dir}&quot; https://raw.githubusercontent.com/udacity/aipnd-project/master/cat_to_name.json with open(os.path.join(data_dir, &#39;cat_to_name.json&#39;), &#39;r&#39;) as f: cat_to_name = json.load(f) . --2020-04-05 12:33:05-- https://raw.githubusercontent.com/udacity/aipnd-project/master/cat_to_name.json Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 2218 (2.2K) [text/plain] Saving to: ‘/content/flower_data/cat_to_name.json’ cat_to_name.json 100%[===================&gt;] 2.17K --.-KB/s in 0s 2020-04-05 12:33:05 (63.8 MB/s) - ‘/content/flower_data/cat_to_name.json’ saved [2218/2218] . # Test the data loader images, labels = next(iter(dataloaders[&#39;train&#39;])) images.size() images, labels = next(iter(dataloaders[&#39;train&#39;])) rand_idx = np.random.randint(len(images)) # print(rand_idx) print(&quot;label: {}, class: {}, name: {}&quot;.format(labels[rand_idx].item(), class_names[labels[rand_idx].item()], cat_to_name[class_names[labels[rand_idx].item()]])) . label: 65, class: 66, name: osteospermum . Build new classifier layers . The classifier layers of the pre-trained network must be replaced with a classifier that fits the parameters of our flower-image classification problem. . Before building and replacing the network classifier, I will exclude all pre-trained parameters from gradient computation. . The input in the new classifier must match the output of the pre-trained network (25,088). The output of the classifier will be the set of image categories in the flowers datasets (102). . . Tip: To find out more about other predefined models from PyTorch, run the following command and refer to this documentation: https://pytorch.org/docs/stable/torchvision/models.html. . dir(models) . Train the classifier and track running performance . The two main tasks in this section are the following: . Train the classifier layers using backpropagation and the pre-trained network to get the features. | Track the loss and accuracy on the validation set to determine the best hyper-parameters. | Train the classifier layers . I start by instantiating the necessary classes for the optimization component of network training. . First, the loss function will be a Negative Log Likelihood (torch.nn.NLLLoss) . Second, the optimization method will be the Adam algorithm (torch.optim.Adam). . # Instantiate loss function loss_function = nn.NLLLoss() # Instantiate optimization algorithm learning_rate = 0.0001 optimizer = optim.Adam(model_conv.classifier.parameters(), lr=learning_rate) # Enable CUDA: use GPUs for model computation if torch.cuda.is_available(): model_conv.to(&#39;cuda&#39;) # Now we define params for the training loop # Epochs: number of iterations over the entire training dataset epochs = 15 # Number of iterations between printing loss and accuracy print_steps = 30 . Now we can define a function for training the model. . Create the optimizer . Now that the model structure is correct, the final step for finetuning and feature extracting is to create an optimizer that only updates the desired parameters. Recall that after loading the pretrained model, but before reshaping, if feature_extract=True we manually set all of the parameter’s .requires_grad attributes to False. Then the reinitialized layer’s parameters have .requires_grad=True by default. So now we know that all parameters that have .requires_grad=True should be optimized. Next, we make a list of such parameters and input this list to the SGD algorithm constructor. . To verify this, check out the printed parameters to learn. When finetuning, this list should be long and include all of the model parameters. However, when feature extracting this list should be short and only include the weights and biases of the reshaped layers. . model_conv = model_conv.to(device) # Gather the parameters to be optimized/updated in this run. If we are # finetuning we will be updating all parameters. However, if we are # doing feature extract method, we will only update the parameters # that we have just initialized, i.e. the parameters with requires_grad # is True. params_to_update = model_conv.parameters() print(&quot;Params to learn:&quot;) if feature_extract: params_to_update = [] for name,param in model_conv.named_parameters(): if param.requires_grad == True: params_to_update.append(param) print(&quot; t&quot;,name) else: for name,param in model_conv.named_parameters(): if param.requires_grad == True: print(&quot; t&quot;,name) criterion = nn.CrossEntropyLoss() # Observe that all parameters are being optimized optimizer_conv = optim.SGD(params_to_update, lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1) . Params to learn: classifier.6.weight classifier.6.bias . Now we are ready to train our model! . model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=3 ) . Epoch 0/2 - train Loss: 3.0470 Acc: 0.3307 valid Loss: 1.8235 Acc: 0.6015 Epoch 1/2 - train Loss: 1.9147 Acc: 0.5449 valid Loss: 1.3099 Acc: 0.6980 Epoch 2/2 - train Loss: 1.5916 Acc: 0.6090 valid Loss: 1.0807 Acc: 0.7494 Training complete in 3m 22s Best val Acc: 0.749389 . Calculate Accuracy . We can also define a function to calculate the accuracy. . def calc_accuracy(mode=&#39;test&#39;): # Initialize validation counters cnt_correct = 0 cnt_total = 0 # no_grad() prevents tracking history (and using memory) with torch.no_grad(): # Iterate over the entire validation dataset for input_images, labels in dataloaders[mode]: # Enable CUDA: use GPUs for model computation if torch.cuda.is_available(): input_images, labels = input_images.to(&#39;cuda&#39;), labels.to(&#39;cuda&#39;) # Make predictions outputs = model_conv(input_images) _, predicted = torch.max(outputs.data, 1) # Count total and correct predictions cnt_total += labels.size(0) cnt_correct += (predicted == labels).sum().item() print(mode + &#39; accuracy ({0:d} images): {1:.1%}&#39; .format(cnt_total, cnt_correct / cnt_total)) . calc_accuracy(&#39;valid&#39;) calc_accuracy(&#39;test&#39;) . valid accuracy (818 images): 74.9% test accuracy (819 images): 70.9% . calc_accuracy(&#39;train&#39;) . train accuracy (6552 images): 72.7% . Visualized model . def visualize_model(model, num_images=6, mode=&#39;test&#39;): was_training = model.training model.eval() images_so_far = 0 fig = plt.figure() with torch.no_grad(): for i, (inputs, labels) in enumerate(dataloaders[mode]): inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) _, preds = torch.max(outputs, 1) for j in range(inputs.size()[0]): images_so_far += 1 ax = plt.subplot(num_images//2, 2, images_so_far) ax.axis(&#39;off&#39;) # ax.axis([0, 224, 0,224]) num_label = class_names[preds[j]] str_label = cat_to_name[num_label] ax.set_title(&#39;{}&#39;.format(str_label)) imshow(inputs.cpu().data[j]) if images_so_far == num_images: model.train(mode=was_training) return model.train(mode=was_training) # out = utils.make_grid(inputs) # imshow(out) . visualize_model(model_conv, mode=&#39;test&#39;) . Save the trained network . Now that the network is trained, I save the model so it can be loaded later to make predictions. . In order to save the model allowing for the option to keep training it later, I also save the necessary training parameters: number of epochs and the optimizer state. . # Save the mapping of the flower labels (1-102) to array indices (0-101) model_conv.class_to_idx = image_datasets[&#39;train&#39;].class_to_idx # Create dictionary with needed components to rebuild model checkpoint = { &#39;model&#39;: model_conv, &#39;epochs&#39;: epochs, &#39;optimizer_state&#39;: optimizer.state_dict, &#39;labels_to_flower_names&#39;: cat_to_name } # Save checkpoint torch.save(checkpoint, &#39;checkpoint_04-05-2020.pth&#39;) . Predict images based on the model . # Stats needed to normalize ImageNet images ImageNet = { &#39;means&#39; : np.array([0.485, 0.456, 0.406]), &#39;std_devs&#39; : np.array([0.229, 0.224, 0.225]), &#39;short_ax_max&#39; : 255, &#39;resize&#39; : 224 } . # Process a PIL image for use in a PyTorch model def process_image(pil_image, image_reqs=ImageNet): &quot;&quot;&quot;Scale, crop, and normalize a PIL image for a PyTorch model. Returns a NumPy array Args: pil_image (PIL image): Input image for a prediction task image_reqs (dict): Image stats needed so image matches model reqs Returns: NumPy array &quot;&quot;&quot; # Resize image so that shortest side is 256 pixels, keeping ratio img_size = pil_image.size ratio = max(img_size) / min(img_size) new_size = [0, 0] short = img_size.index(min(img_size)) long = 1 - short new_size[short] = image_reqs[&#39;short_ax_max&#39;] new_size[long] = int(image_reqs[&#39;short_ax_max&#39;] * ratio) pil_image = pil_image.resize(size=tuple(new_size)) # Crop out the center 224x224 portion of the image gap_x = int((new_size[0] - image_reqs[&#39;resize&#39;]) / 2) gap_y = int((new_size[1] - image_reqs[&#39;resize&#39;]) / 2) crop_box = (gap_x, gap_y, gap_x + image_reqs[&#39;resize&#39;], gap_y + image_reqs[&#39;resize&#39;]) pil_image = pil_image.crop(box=crop_box) # Re-encode image color channels np_image = np.array(pil_image) / 255 # Normalize image accordingly to the same statistics used to train norm_image = (np_image - image_reqs[&#39;means&#39;]) / image_reqs[&#39;std_devs&#39;] # Reorder dimensions of NumPy array so it matches PyTorch&#39;s input norm_image = norm_image.transpose((2, 0, 1)) return norm_image . Deploy trained network for predictions . Now that images are processed to the expected PyTorch format, they can be passed into a function that uses the trained model to make predictions. . A common practice is to predict the top 5, or so, most probable classes (usually called top-$K$). The function below, predict(), takes the image filepath and the trained model — and returns the ordered lists of the most likely classes and their corresponding probabilities. . # Load a flower image and make a prediction of the top-K classes def predict(img_filepath, model, topk=5): &#39;&#39;&#39; Predict the top-K classes of an image using a trained deep learning model. Args: img_filepath (str): Input image for the prediction task model (torchvision.models): Trained deep learning model topk (int): Number of top most likely predictions Returns: top_probs (list) classes (list) &#39;&#39;&#39; # Open PIL image pil_image = Image.open(img_filepath) # Scale, crop, normalize PIL image np_image = process_image(pil_image, image_reqs=ImageNet) # Resize NumPy array to match dataloader output np_image = np.resize(np_image,(1, 3, 224, 224)) # Covert NumPy array to PyTorch tensor img_tensor = torch.from_numpy(np_image).type(torch.FloatTensor) # Enable CUDA: use GPUs for model computation if torch.cuda.is_available(): img_tensor = img_tensor.to(&#39;cuda&#39;) # Run model in evaluation mode model.eval() with torch.no_grad(): outputs = model(img_tensor) # Convert softmax output to probabilities probs = torch.exp(outputs.data) # Find top-k probabilities and indices top_probs, indices = torch.topk(probs, dim=1, k=topk) # Convert PyTorch tensors to lists top_probs, indices = top_probs.to(&#39;cpu&#39;).numpy(), indices.to(&#39;cpu&#39;).numpy() top_probs, indices = top_probs[0].tolist(), indices[0].tolist() # Find the class using the indices (reverse dictionary first) idx_to_class = {idx: class_ for class_, idx in model.class_to_idx.items()} classes = [idx_to_class[i] for i in indices] return top_probs, classes . Sanity check: Make a flower category prediction . In this final section we will check whether the predictions that come out of the image classification tool make sense. . This will not be a comprehensive test, but just a sanity check to screen for obvious bugs and errors. . The main steps in this prediction test are the following: . Choose a flower image, of known category, from the test set. And run the image through the trained network using the predict() function. . | Get the flower names (in words) from the checkpoint dictionary, for the chosen test flower and the predicted top-$K$ classes. . | Write the function imshow(), which takes the output of the process_image() function and re-processes the array to be plotted with matplotlib. This will let us visualize the input image as seen by the trained model. . | Display the test flower image along with a bar chart of the top-5 predicted categories. . | Choose test image and run through the model. . # Choose a flower category and individual image for prediction example test_class = &#39;74&#39; pathname = data_dir+&#39;/test/&#39;+test_class+&quot;/&quot; import random rand_file = random.choice(os.listdir(pathname)) filepath = pathname+rand_file # print(filepath) # Run predict() to get model predictions: top-k classes and probabilities probs, classes = predict(filepath, model_conv, topk=5) . Get flower names from checkpoint dictionary . # Get the flower-category names from the saved checkpoint flower_names = checkpoint[&#39;labels_to_flower_names&#39;] test_flower_name = flower_names[test_class] top_k_names = [flower_names[key] for key in classes] . def imshow(image, ax=None, title=None, image_reqs=ImageNet): if ax is None: fig, ax = plt.subplots() # PyTorch tensors assume the color channel is the first dimension # but matplotlib assumes is the third dimension image = image.transpose((1, 2, 0)) # Undo preprocessing mean = image_reqs[&#39;means&#39;] std = image_reqs[&#39;std_devs&#39;] image = std * image + mean # Image needs to be clipped between 0 and 1 image = np.clip(image, 0, 1) ax.imshow(image) . # Create figure and axes fig, (flower, prob_bars) = plt.subplots(nrows=2, ncols=1, figsize=(6,8)) # Render the flower image imshow(image=process_image(Image.open(filepath)), ax=flower) flower.set_title(&#39;Test flower category: n— {} —&#39;.format(test_flower_name), fontsize=16) # Make barchart of top-k probabilities index = np.arange(len(classes))[::-1] prob_bars.barh(index, probs, tick_label=top_k_names, align=&#39;center&#39;) prob_bars.set_title(&#39;Model prediction&#39;, fontsize=14) prob_bars.set_xlabel(&#39;Probability&#39;, fontsize=12) prob_bars.set_ylabel(&#39;Top-K classes&#39;, fontsize=12) # Plot the final figure fig.tight_layout() . Conclusion . We have briefly covered the fundamentals of image classification with PyTorch, from transfer learning until prediction. Now, let&#39;s go one step deeper to object detection. . Reference . This notebook is a compilation of multiple information taken from the following different sources. Credits go to their respective authors. . https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html | https://cs231n.github.io/transfer-learning/ | https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html | https://medium.com/datadriveninvestor/creating-a-pytorch-image-classifier-da9db139ba80 | https://github.com/jclh/image-classifier-PyTorch/blob/master/flower-classifier-PyTorch.ipynb | .",
            "url": "https://noxouille.github.io/pydata2020-3LVDL/image%20classification/2020/04/16/level1.html",
            "relUrl": "/image%20classification/2020/04/16/level1.html",
            "date": " • Apr 16, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Level 2 - Object Detection",
            "content": "Compiled by: Ferdi Pratama . Overview . Main idea . Instead of classifying just one single image as a whole, we now consider localization. This allows us to identify multiple possible regions of particular objects within an image. . . Source: https://thumbs.gfycat.com/PowerfulSaltyCapeghostfrog-size_restricted.gif . Existing architecture . SSD: Single Shot MultiBox Detector | Deep Residual Network (ResNet) | You Only Look Once (YOLO): Unified, Real-Time Object Detection | R-CNN | Fast R-CNN | Faster R-CNN | OverFeat | ... and many more! | . Load an SSD model . The command below will load an SSD model pretrained on COCO dataset from Torch Hub. . Setting precision = fp16 will load a checkpoint trained with mixed precision into architecture enabling execution on Tensor Cores. Handling mixed precision data requires Apex library. . import torch precision = &#39;fp32&#39; ssd_model = torch.hub.load(&#39;NVIDIA/DeepLearningExamples:torchhub&#39;, &#39;nvidia_ssd&#39;, model_math=precision) . Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub . Torch Hub . TODO: explain torch hub . For convenient and comprehensive formatting of input and output of the model, load a set of utility methods. . utils = torch.hub.load(&#39;NVIDIA/DeepLearningExamples:torchhub&#39;, &#39;nvidia_ssd_processing_utils&#39;) . Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub . Prepare the model for inference . ssd_model.to(&#39;cuda&#39;) ssd_model.eval() . SSD300( (feature_extractor): ResNet( (feature_extractor): Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): Bottleneck( (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (5): Sequential( (0): Bottleneck( (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (6): Sequential( (0): Bottleneck( (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (4): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (5): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) ) ) (additional_blocks): ModuleList( (0): Sequential( (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (1): Sequential( (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (2): Sequential( (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (3): Sequential( (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), bias=False) (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) (4): Sequential( (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), bias=False) (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) ) ) (loc): ModuleList( (0): Conv2d(1024, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (5): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) ) (conf): ModuleList( (0): Conv2d(1024, 324, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): Conv2d(512, 486, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (2): Conv2d(512, 486, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): Conv2d(256, 486, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): Conv2d(256, 324, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (5): Conv2d(256, 324, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) ) ) . Define test images . Below, we add some URLs of images that we want to test. . uris = [ &#39;https://c8.alamy.com/comp/X2ND8M/couple-eating-pizza-in-living-room-X2ND8M.jpg&#39;, &#39;https://www.saltwire.com/media/photologue/photos/cache/RisingSeas-story7sidebar-fcampbellHeavyTrafficHfx2678_large.jpg&#39;, &#39;https://www.reynoldskitchens.com/sites/default/files/styles/hero_image/public/recipes/broccoli-carrots-with-oranges_1.jpg?itok=0ZbTVFho&#39; ] . Format the images to comply with the network input and convert them to tensor. . inputs = [utils.prepare_input(uri) for uri in uris] tensor = utils.prepare_tensor(inputs, precision == &#39;fp16&#39;) . Run the SSD network . Finally, we can perform object detection. . with torch.no_grad(): detections_batch = ssd_model(tensor) . By default, raw output from SSD network per input image contains 8732 boxes with localization and class probability distribution. Let&#39;s filter this output to only get reasonable detections (confidence&gt;40%) in a more comprehensive format. . results_per_input = utils.decode_results(detections_batch) best_results_per_input = [utils.pick_best(results, 0.40) for results in results_per_input] . The model was trained on COCO dataset, which we need to access in order to translate class IDs into object names. For the first time, downloading annotations may take a while. . classes_to_labels = utils.get_coco_object_dictionary() . Check out the label definitions . print(&#39; n&#39;.join(classes_to_labels)) . person bicycle car motorcycle airplane bus train truck boat traffic light fire hydrant stop sign parking meter bench bird cat dog horse sheep cow elephant bear zebra giraffe backpack umbrella handbag tie suitcase frisbee skis snowboard sports ball kite baseball bat baseball glove skateboard surfboard tennis racket bottle wine glass cup fork knife spoon bowl banana apple sandwich orange broccoli carrot hot dog pizza donut cake chair couch potted plant bed dining table toilet tv laptop mouse remote keyboard cell phone microwave oven toaster sink refrigerator book clock vase scissors teddy bear hair drier toothbrush . Visualize the results . First, we generate random colors, so that we get different color for every class label. . import colorsys import random def random_colors(N, bright=True): &quot;&quot;&quot; Generate random colors. To get visually distinct colors, generate them in HSV space then convert to RGB. &quot;&quot;&quot; brightness = 1.0 if bright else 0.7 hsv = [(i / N, 1, brightness) for i in range(N)] colors = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv)) random.shuffle(colors) return colors colors = random_colors(len(classes_to_labels)) . from matplotlib import pyplot as plt import matplotlib.patches as patches fig, axs = plt.subplots(1, 3, figsize=(10, 4), dpi=150) for ax, image_idx in zip(axs, range(len(best_results_per_input))): # Show original, denormalized image... image = inputs[image_idx] / 2 + 0.5 ax.imshow(image) ax.axis(&#39;off&#39;) # ...with detections bboxes, classes, confidences = best_results_per_input[image_idx] for idx in range(len(bboxes)): left, bot, right, top = bboxes[idx] x, y, w, h = [val * 300 for val in [left, bot, right - left, top - bot]] rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor=colors[classes[idx]], facecolor=&#39;none&#39;) ax.add_patch(rect) ax.text(x, y, &quot;{} {:.0f}%&quot;.format(classes_to_labels[classes[idx] - 1], confidences[idx]*100), bbox=dict(facecolor=colors[classes[idx]], alpha=0.4), fontsize=13) plt.show() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . Conclusion . TODO: add conclusion . References . https://pytorch.org/hub/nvidia_deeplearningexamples_ssd/ https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD | https://towardsdatascience.com/object-detection-and-tracking-in-pytorch-b3cf1a696a98 | .",
            "url": "https://noxouille.github.io/pydata2020-3LVDL/object%20detection/2020/04/15/level2.html",
            "relUrl": "/object%20detection/2020/04/15/level2.html",
            "date": " • Apr 15, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Level 3 - Instance Segmentation",
            "content": "Compiled by: Ferdi Pratama . Overview . For this level 3 of our Vision Deep Learning session, we will explore image segmentation. One of the best Pytorch implementations of Mask R-CNN I would recommend you to check out is Detectron2. . . Detectron2 is developed by Facebook AI research, it contains efficient implementation of algorithms related to object detections, such as object detection, image segmentation, pose estimation, and other cool stuffs! . . Installation . Let&#39;s install detectron2 in our notebook! . #collapse !pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu100/index.html . . Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu100/index.html Collecting detectron2 Downloading https://dl.fbaipublicfiles.com/detectron2/wheels/cu100/detectron2-0.1.1%2Bcu100-cp36-cp36m-linux_x86_64.whl (6.2MB) |████████████████████████████████| 6.2MB 699kB/s Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from detectron2) (3.2.1) Requirement already satisfied: tqdm&gt;4.29.0 in /usr/local/lib/python3.6/dist-packages (from detectron2) (4.38.0) Requirement already satisfied: pydot in /usr/local/lib/python3.6/dist-packages (from detectron2) (1.3.0) Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from detectron2) (1.3.0) Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from detectron2) (7.0.0) Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from detectron2) (0.16.0) Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from detectron2) (2.2.0) Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from detectron2) (0.8.7) Collecting fvcore Downloading https://files.pythonhosted.org/packages/46/d5/919fece098c0aa5c3efd040f729dc02570df04e6a659ce6547ade1ea531c/fvcore-0.1.dev200416.tar.gz Requirement already satisfied: termcolor&gt;=1.1 in /usr/local/lib/python3.6/dist-packages (from detectron2) (1.1.0) Collecting yacs&gt;=0.1.6 Downloading https://files.pythonhosted.org/packages/2f/51/9d613d67a8561a0cdf696c3909870f157ed85617fea3cff769bb7de09ef7/yacs-0.1.6-py3-none-any.whl Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;detectron2) (0.10.0) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;detectron2) (2.8.1) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;detectron2) (2.4.7) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;detectron2) (1.2.0) Requirement already satisfied: numpy&gt;=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;detectron2) (1.18.2) Requirement already satisfied: six&gt;=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard-&gt;detectron2) (1.12.0) Requirement already satisfied: google-auth&lt;2,&gt;=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard-&gt;detectron2) (1.7.2) Requirement already satisfied: grpcio&gt;=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard-&gt;detectron2) (1.28.1) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard-&gt;detectron2) (3.2.1) Requirement already satisfied: wheel&gt;=0.26; python_version &gt;= &#34;3&#34; in /usr/local/lib/python3.6/dist-packages (from tensorboard-&gt;detectron2) (0.34.2) Requirement already satisfied: werkzeug&gt;=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard-&gt;detectron2) (1.0.1) Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard-&gt;detectron2) (0.4.1) Requirement already satisfied: requests&lt;3,&gt;=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard-&gt;detectron2) (2.21.0) Requirement already satisfied: protobuf&gt;=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard-&gt;detectron2) (3.10.0) Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard-&gt;detectron2) (1.6.0.post3) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard-&gt;detectron2) (46.1.3) Requirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard-&gt;detectron2) (0.9.0) Collecting pyyaml&gt;=5.1 Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB) |████████████████████████████████| 276kB 10.7MB/s Collecting portalocker Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl Requirement already satisfied: cachetools&lt;3.2,&gt;=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard-&gt;detectron2) (3.1.1) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard-&gt;detectron2) (0.2.8) Requirement already satisfied: rsa&lt;4.1,&gt;=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard-&gt;detectron2) (4.0) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard-&gt;detectron2) (1.3.0) Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard-&gt;detectron2) (3.0.4) Requirement already satisfied: idna&lt;2.9,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard-&gt;detectron2) (2.8) Requirement already satisfied: urllib3&lt;1.25,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard-&gt;detectron2) (1.24.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard-&gt;detectron2) (2020.4.5.1) Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard-&gt;detectron2) (0.4.8) Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard-&gt;detectron2) (3.1.0) Building wheels for collected packages: fvcore, pyyaml Building wheel for fvcore (setup.py) ... done Created wheel for fvcore: filename=fvcore-0.1.dev200416-cp36-none-any.whl size=39838 sha256=162a7e943dff18e79c1251543bd8d92c592d20aeaa2ff8e8ca3e466d09bae17c Stored in directory: /root/.cache/pip/wheels/7f/ad/82/340ad5c00feff69f2a7dcca701c6099a0cc9f93a31bacc70bd Building wheel for pyyaml (setup.py) ... done Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=ad52e0e90e9ce95c977b73de3ffbf179d829a6172efa98c6547000334615d167 Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd Successfully built fvcore pyyaml Installing collected packages: pyyaml, yacs, portalocker, fvcore, detectron2 Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Successfully installed detectron2-0.1.1+cu100 fvcore-0.1.dev200416 portalocker-1.7.0 pyyaml-5.3.1 yacs-0.1.6 . Basic setup . # You may need to restart your runtime prior to this, to let your installation take effect # Some basic setup: # Setup detectron2 logger import detectron2 from detectron2.utils.logger import setup_logger setup_logger() # import some common libraries import numpy as np import cv2 import random from google.colab.patches import cv2_imshow # import some common detectron2 utilities from detectron2 import model_zoo from detectron2.engine import DefaultPredictor from detectron2.config import get_cfg from detectron2.utils.visualizer import Visualizer from detectron2.data import MetadataCatalog . Run a pre-trained model . We first download a random image from the COCO dataset: . !wget http://images.cocodataset.org/val2017/000000439715.jpg -O input.jpg im = cv2.imread(&quot;./input.jpg&quot;) cv2_imshow(im) . --2020-04-16 01:39:50-- http://images.cocodataset.org/val2017/000000439715.jpg Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.25.4 Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.25.4|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 209222 (204K) [image/jpeg] Saving to: ‘input.jpg’ input.jpg 100%[===================&gt;] 204.32K 719KB/s in 0.3s 2020-04-16 01:39:50 (719 KB/s) - ‘input.jpg’ saved [209222/209222] . Then, we create a detectron2 config and a detectron2 DefaultPredictor to run inference on this image. . cfg = get_cfg() # add project-specific config (e.g., TensorMask) here if you&#39;re not running a model in detectron2&#39;s core library cfg.merge_from_file(model_zoo.get_config_file(&quot;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;)) cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 # set threshold for this model # Find a model from detectron2&#39;s model zoo. You can use the https://dl.fbaipublicfiles... url as well cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(&quot;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;) predictor = DefaultPredictor(cfg) outputs = predictor(im) . model_final_f10217.pkl: 178MB [00:16, 10.6MB/s] . # look at the outputs. See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification outputs[&quot;instances&quot;].pred_classes outputs[&quot;instances&quot;].pred_boxes . Boxes(tensor([[126.6035, 244.8977, 459.8291, 480.0000], [251.1083, 157.8127, 338.9731, 413.6379], [114.8496, 268.6864, 148.2352, 398.8111], [ 0.8217, 281.0327, 78.6072, 478.4210], [ 49.3954, 274.1229, 80.1545, 342.9808], [561.2248, 271.5816, 596.2755, 385.2552], [385.9072, 270.3125, 413.7130, 304.0397], [515.9295, 278.3744, 562.2792, 389.3802], [335.2409, 251.9167, 414.7491, 275.9375], [350.9300, 269.2060, 386.0984, 297.9081], [331.6292, 230.9996, 393.2759, 257.2009], [510.7349, 263.2656, 570.9865, 295.9194], [409.0841, 271.8646, 460.5582, 356.8722], [506.8767, 283.3257, 529.9403, 324.0392], [594.5663, 283.4820, 609.0577, 311.4124]], device=&#39;cuda:0&#39;)) . Visualize the result . # We can use `Visualizer` to draw the predictions on the image. v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2) v = v.draw_instance_predictions(outputs[&quot;instances&quot;].to(&quot;cpu&quot;)) cv2_imshow(v.get_image()[:, :, ::-1]) . Train on a custom dataset . TODO: add contents . Conclusion . TODO: add conclusion . Reference . https://github.com/facebookresearch/detectron2 | . TODO: add more references .",
            "url": "https://noxouille.github.io/pydata2020-3LVDL/instance%20segmentation/2020/04/14/level3.html",
            "relUrl": "/instance%20segmentation/2020/04/14/level3.html",
            "date": " • Apr 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Ferdi Pratama. I have a little bit of experience in AI and deep learning. Currently focusing my career in Business Intelligence and Data Science. .",
          "url": "https://noxouille.github.io/pydata2020-3LVDL/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://noxouille.github.io/pydata2020-3LVDL/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}